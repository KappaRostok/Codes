{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/DataSets/Kaggle/cis-Fraud/train_transaction.csv',index_col='TransactionID')\n",
    "train_ident = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/DataSets/Kaggle/cis-Fraud/train_identity.csv',index_col='TransactionID')\n",
    "\n",
    "test_ident = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/DataSets/Kaggle/cis-Fraud/test_identity.csv',index_col='TransactionID')\n",
    "test_data = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/DataSets/Kaggle/cis-Fraud/test_transaction.csv',index_col='TransactionID')\n",
    "\n",
    "train_data = train_data.merge(train_ident, how='left', left_index=True, right_index=True)\n",
    "test_data = test_data.merge(test_ident, how='left', left_index=True, right_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_value_cols = [col for col in train_data.columns if train_data[col].nunique() <= 1] \n",
    "\n",
    "many_null_cols = [col for col in train_data.columns if train_data[col].isnull().sum() / train_data.shape[0] > 0.9] \n",
    "\n",
    "big_top_value_cols = [col for col in train_data.columns if train_data[col].value_counts(dropna=False, normalize=True).values[0] > 0.9] \n",
    "\n",
    "cols_to_drop = list(set(many_null_cols + big_top_value_cols + one_value_cols)) \n",
    "\n",
    "cols_to_drop.remove('isFraud') \n",
    "\n",
    "train_data = train_data.drop(cols_to_drop, axis=1)\n",
    "test_data = test_data.drop(cols_to_drop, axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "big_data=pd.concat([train_data,test_data])\n",
    "\n",
    "big_data[\"ProductCD\"] = big_data[\"ProductCD\"].map({\"W\":1, \"H\":2, \"C\" : 3 , \"S\":4, \"R\":5})\n",
    "big_data[\"card4\"] = big_data[\"card4\"].map({\"discover\":1, \"mastercard\":2, \"visa\" : 3 , \"american express\":4})\n",
    "big_data[\"card6\"] = big_data[\"card6\"].map({\"debit\":1, \"credit\":2, \"debit or credit\" : 3 , \"charge card\":4})\n",
    "big_data[\"M2\"] = big_data[\"M2\"].map({\"T\":1, \"F\":2})\n",
    "big_data[\"M3\"] = big_data[\"M3\"].map({\"T\":1, \"F\":2})\n",
    "big_data[\"M3\"] = big_data[\"M3\"].map({\"T\":1, \"F\":2})\n",
    "big_data[\"M4\"] = big_data[\"M4\"].map({\"M0\":1, \"M1\":2, \"M2\" : 3})\n",
    "big_data[\"M5\"] = big_data[\"M5\"].map({\"T\":1, \"F\":2})\n",
    "big_data[\"M6\"] = big_data[\"M6\"].map({\"T\":1, \"F\":2})\n",
    "big_data[\"M7\"] = big_data[\"M7\"].map({\"T\":1, \"F\":2})\n",
    "big_data[\"M8\"] = big_data[\"M8\"].map({\"T\":1, \"F\":2})\n",
    "big_data[\"M9\"] = big_data[\"M9\"].map({\"T\":1, \"F\":2})\n",
    "\n",
    "big_data[\"id_12\"] = big_data[\"id_12\"].map({\"NotFound\":1, \"Found\":2})\n",
    "big_data[\"id_15\"] = big_data[\"id_15\"].map({\"New\":1, \"Found\":2, \"Unknown\" : 3})\n",
    "big_data[\"id_36\"] = big_data[\"id_36\"].map({\"T\":1, \"F\":2})\n",
    "big_data[\"id_37\"] = big_data[\"id_37\"].map({\"T\":1, \"F\":2})\n",
    "big_data[\"id_38\"] = big_data[\"id_38\"].map({\"T\":1, \"F\":2})\n",
    "big_data[\"DeviceType\"] = big_data[\"DeviceType\"].map({\"mobile\":1, \"desktop\":2})\n",
    "\n",
    "big_data['id_33']=big_data['id_33'].astype(str)\n",
    "big_data['id_33'] = big_data['id_33'].apply(lambda x: int(x.split('x')[0])*int(x.split('x')[1]) if x!='nan' else -999)\n",
    "big_data['id_33']=big_data['id_33'].astype(int)\n",
    "\n",
    "big_data['TransactionAmt_decimal'] = ((big_data['TransactionAmt'] - big_data['TransactionAmt'].astype(int)) * 1000).astype(int)\n",
    "\n",
    "big_data['Transaction_day_of_week'] = np.floor((big_data['TransactionDT'] / (3600 * 24) - 1) % 7)\n",
    "big_data['Transaction_hour'] = np.floor(big_data['TransactionDT'] / 3600) % 24\n",
    "\n",
    "big_data['P_emaildomain']=big_data['P_emaildomain'].fillna('None')\n",
    "big_data['onlyhostsP']=big_data['P_emaildomain'].apply(lambda x: x.split('.')[0] if '.' in x else x)\n",
    "big_data['onlylocP']=big_data['P_emaildomain'].apply(lambda x: x.split('.')[1] if '.' in x else 'None')\n",
    "\n",
    "big_data['R_emaildomain']=big_data['R_emaildomain'].fillna('None')\n",
    "big_data['onlyhostsR']=big_data['R_emaildomain'].apply(lambda x: x.split('.')[0] if '.' in x else x)\n",
    "big_data['onlylocR']=big_data['R_emaildomain'].apply(lambda x: x.split('.')[1] if '.' in x else 'None')\n",
    "\n",
    "big_data['id_30']=big_data['id_30'].fillna('None')\n",
    "big_data['id_30']=big_data['id_30'].apply(lambda x: x.split(' ')[0] if ' ' in x else x)\n",
    "\n",
    "big_data['id_31']=big_data['id_31'].fillna('None')\n",
    "big_data['id_31']=(big_data['id_31'].apply(lambda x: x.split(' ')[0] if ' ' in x else x)).apply(lambda x: x.split('/')[0] if '/' in x else x)\n",
    "\n",
    "big_data['D9'] = (big_data['TransactionDT']%(3600*24)/3600//1)/24.0\n",
    "\n",
    "\n",
    "big_data['card1_count_full'] = big_data['card1'].map(big_data['card1'].value_counts(dropna=False))\n",
    "\n",
    "for feature in ['id_02__id_20', 'id_02__D8', 'D11__DeviceInfo', 'DeviceInfo__P_emaildomain', 'P_emaildomain__C2', \n",
    "                'card2__dist1', 'card1__card5', 'card2__id_20', 'card5__P_emaildomain', 'addr1__card1']:\n",
    "\n",
    "    f1, f2 = feature.split('__')\n",
    "    big_data[feature] = big_data[f1].astype(str) + '_' + big_data[f2].astype(str)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    le.fit(list(big_data[feature].astype(str).values))\n",
    "    big_data[feature] = le.transform(list(big_data[feature].astype(str).values))\n",
    "    \n",
    "    \n",
    "for feature in ['id_34', 'id_36']:\n",
    "        big_data[feature + '_count_full'] = big_data[feature].map(big_data[feature].value_counts(dropna=False))\n",
    "         \n",
    "for feature in ['id_01', 'id_31', 'id_33', 'id_35', 'id_36']:\n",
    "        big_data[feature + '_count_dist'] = big_data[feature].map(big_data[feature].value_counts(dropna=False))\n",
    "\n",
    "\n",
    "\n",
    "big_data['DeviceInfo']=big_data['DeviceInfo'].fillna('None')\n",
    "big_data['None?']=big_data['DeviceInfo'].apply(lambda x: 1 if 'None' in x else 0)\n",
    "big_data['Windows?']=big_data['DeviceInfo'].apply(lambda x: 1 if 'Windows' in x else 0)\n",
    "big_data['iOS?']=big_data['DeviceInfo'].apply(lambda x: 1 if 'iOS' in x else 0)\n",
    "big_data['MacOS?']=big_data['DeviceInfo'].apply(lambda x: 1 if 'MacOS' in x else 0)\n",
    "big_data['SM?']=big_data['DeviceInfo'].apply(lambda x: 1 if 'SM' in x else 0)\n",
    "big_data['Trident?']=big_data['DeviceInfo'].apply(lambda x: 1 if 'Trident' in x else 0)\n",
    "big_data['SAMSUNG?']=big_data['DeviceInfo'].apply(lambda x: 1 if 'SAMSUNG' in x else 0)\n",
    "big_data['Moto?']=big_data['DeviceInfo'].apply(lambda x: 1 if 'Moto' in x else 0)\n",
    "big_data['LG?']=big_data['DeviceInfo'].apply(lambda x: 1 if 'LG' in x else 0)\n",
    "big_data['rv:?']=big_data['DeviceInfo'].apply(lambda x: 1 if 'rv:' in x else 0)\n",
    "big_data['HUAWEI?']=big_data['DeviceInfo'].apply(lambda x: 1 if 'HUAWEI' in x else 0)\n",
    "big_data['HTC?']=big_data['DeviceInfo'].apply(lambda x: 1 if 'HTC' in x else 0)\n",
    "big_data['ALE?']=big_data['DeviceInfo'].apply(lambda x: 1 if 'ALE' in x else 0)\n",
    "big_data['Lenovo?']=big_data['DeviceInfo'].apply(lambda x: 1 if 'Lenovo' in x else 0)\n",
    "big_data['Blade?']=big_data['DeviceInfo'].apply(lambda x: 1 if 'Blade' in x else 0)\n",
    "big_data['Pixel?']=big_data['DeviceInfo'].apply(lambda x: 1 if 'Pixel' in x else 0)\n",
    "big_data['Ilium ?']=big_data['DeviceInfo'].apply(lambda x: 1 if 'Ilium ' in x else 0)\n",
    "big_data['Hisense?']=big_data['DeviceInfo'].apply(lambda x: 1 if 'Hisense' in x else 0)\n",
    "big_data['hi6210sft?']=big_data['DeviceInfo'].apply(lambda x: 1 if 'hi6210sft' in x else 0)\n",
    "\n",
    "\n",
    "big_data['TransactionAmt_to_mean_card1'] = big_data['TransactionAmt'] / big_data.groupby(['card1'])['TransactionAmt'].transform('mean')\n",
    "big_data['TransactionAmt_to_mean_card4'] = big_data['TransactionAmt'] / big_data.groupby(['card4'])['TransactionAmt'].transform('mean')\n",
    "big_data['TransactionAmt_to_std_card1'] = big_data['TransactionAmt'] / big_data.groupby(['card1'])['TransactionAmt'].transform('std')\n",
    "big_data['TransactionAmt_to_std_card4'] = big_data['TransactionAmt'] / big_data.groupby(['card4'])['TransactionAmt'].transform('std')\n",
    "\n",
    "big_data['id_02_to_mean_card1'] = big_data['id_02'] / big_data.groupby(['card1'])['id_02'].transform('mean')\n",
    "big_data['id_02_to_mean_card4'] = big_data['id_02'] / big_data.groupby(['card4'])['id_02'].transform('mean')\n",
    "big_data['id_02_to_std_card1'] = big_data['id_02'] / big_data.groupby(['card1'])['id_02'].transform('std')\n",
    "big_data['id_02_to_std_card4'] = big_data['id_02'] / big_data.groupby(['card4'])['id_02'].transform('std')\n",
    "\n",
    "big_data['D15_to_mean_card1'] = big_data['D15'] / big_data.groupby(['card1'])['D15'].transform('mean')\n",
    "big_data['D15_to_mean_card4'] = big_data['D15'] / big_data.groupby(['card4'])['D15'].transform('mean')\n",
    "big_data['D15_to_std_card1'] = big_data['D15'] / big_data.groupby(['card1'])['D15'].transform('std')\n",
    "big_data['D15_to_std_card4'] = big_data['D15'] / big_data.groupby(['card4'])['D15'].transform('std')\n",
    "\n",
    "\n",
    "big_data['D15_to_mean_addr1'] = big_data['D15'] / big_data.groupby(['addr1'])['D15'].transform('mean')\n",
    "big_data['D15_to_mean_card4'] = big_data['D15'] / big_data.groupby(['card4'])['D15'].transform('mean')\n",
    "big_data['D15_to_std_addr1'] = big_data['D15'] / big_data.groupby(['addr1'])['D15'].transform('std')\n",
    "big_data['D15_to_std_card4'] = big_data['D15'] / big_data.groupby(['card4'])['D15'].transform('std')\n",
    "\n",
    "START_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')\n",
    "big_data['DT_M'] = big_data['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n",
    "big_data['DT_M'] = (big_data['DT_M'].dt.year-2017)*12 + big_data['DT_M'].dt.month \n",
    "\n",
    "big_data['TransactionAmt_check'] = np.where(big_data['TransactionAmt'].isin(big_data['TransactionAmt']), 1, 0)\n",
    "\n",
    "\n",
    "#newFeatures---------------------------------------------\n",
    "big_data['new_adrr2_corr<']=big_data['addr2'].apply(lambda x:1 if x<90 else 0)\n",
    "big_data['new_adrr1_corr<']=big_data['addr1'].apply(lambda x:1 if x<536 else 0)\n",
    "big_data['new_card2_corr<']=big_data['card2'].apply(lambda x:1 if x<599 else 0)\n",
    "big_data['new_adrr2_corr>']=big_data['addr2'].apply(lambda x:1 if x>68 else 0)\n",
    "big_data['new_adrr2_corr><']=big_data['addr2'].apply(lambda x:1 if x>68 and x<90 else 0)\n",
    "big_data['new_card5_corr>']=big_data['card5'].apply(lambda x:1 if x>162 else 0)\n",
    "big_data['new_Transaction_hour_corr>']=big_data['Transaction_hour'].apply(lambda x:1 if x>11 else 0)\n",
    "#--------------------------------------------------------\n",
    "cols_to_drop=['P_emaildomain','R_emaildomain','DeviceInfo']\n",
    "\n",
    "big_data = big_data.drop(cols_to_drop, axis=1)\n",
    "\n",
    "\n",
    "forencode=big_data.select_dtypes('object')\n",
    "\n",
    "encoded=pd.get_dummies(forencode)\n",
    "\n",
    "#drop needed col\n",
    "for i in list(forencode):\n",
    "    big_data = big_data.drop(i,axis = 1)\n",
    "# Join the encoded df\n",
    "\n",
    "big_data = big_data.join(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = big_data.iloc[:train_data.shape[0],]\n",
    "test_data = big_data.iloc[train_data.shape[0]:,]\n",
    "\n",
    "train_data=train_data.fillna(-999)\n",
    "test_data=test_data.fillna(-999)\n",
    "\n",
    "y=train_data['isFraud']\n",
    "train_data=train_data.drop('isFraud',axis=1)\n",
    "test_data=test_data.drop('isFraud',axis=1)\n",
    "\n",
    "train_data=train_data.drop('TransactionDT',axis=1)\n",
    "test_data=test_data.drop('TransactionDT',axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, TimeSeriesSplit, StratifiedKFold\n",
    "import lightgbm as lgb\n",
    "\n",
    "n_fold = 10\n",
    "#folds = KFold(n_splits=n_fold,shuffle=True,random_state=42)\n",
    "folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "sample_submission = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/DataSets/Kaggle/cis-Fraud/sample_submission.csv')\n",
    "\n",
    "xgb_submission=sample_submission.copy()\n",
    "xgb_submission['isFraud'] = 0\n",
    "t=0\n",
    "\n",
    "mean=[]\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "#for fold_n, (train_index, valid_index) in enumerate(folds.split(train_data)):\n",
    "for train_index, valid_index in folds.split(train_data, y):\n",
    "  #if t<3:\n",
    "    lgbR=lgb.LGBMClassifier(\n",
    "                objective='binary',\n",
    "                boosting_type='gbdt',\n",
    "                metric='auc',\n",
    "                n_jobs=-1,\n",
    "                learning_rate=0.064,\n",
    "                num_leaves= 2**8,\n",
    "                min_data_in_leaf=1000,\n",
    "                max_depth=10,\n",
    "                tree_learner='serial',\n",
    "                colsample_bytree= 0.85,\n",
    "                subsample_freq=1,\n",
    "                subsample=0.85,\n",
    "                n_estimators=2**9,\n",
    "                max_bin=255,\n",
    "                verbose=-1,\n",
    "                seed= 47,\n",
    "                early_stopping_rounds=100,\n",
    "                reg_alpha=0.3,\n",
    "                reg_lamdba=0.243\n",
    "      )\n",
    "    \n",
    "    X_train_, X_valid = train_data.iloc[train_index], train_data.iloc[valid_index]\n",
    "    y_train_, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "    lgbR.fit(X_train_,y_train_,eval_metric='auc',eval_set=[(X_valid,y_valid)],verbose=50,early_stopping_rounds=100)\n",
    "    del X_train_,y_train_\n",
    "    pred=lgbR.predict_proba(test_data)[:,1]\n",
    "    val=lgbR.predict_proba(X_valid)[:,1]\n",
    "    del X_valid\n",
    "    del lgbR\n",
    "    print('ROC accuracy: {}'.format(roc_auc_score(y_valid, val)))\n",
    "    mean.append(roc_auc_score(y_valid, val))\n",
    "    del val,y_valid\n",
    "    xgb_submission['isFraud'] = xgb_submission['isFraud']+pred/n_fold\n",
    "    del pred\n",
    "    t=t+1\n",
    "\n",
    "m=0\n",
    "for i in mean:\n",
    "    m=m+i/len(mean)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from google.colab import files\n",
    "xgb_submission.to_csv('predict2.csv',index=False) \n",
    "files.download('predict2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# popitka v NN #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#neural nets test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_data, \n",
    "    y, \n",
    "    test_size=0.3,\n",
    "    random_state=42, \n",
    "    shuffle=True)\n",
    "\n",
    "X_train=X_train.values\n",
    "y_train=y_train.values\n",
    "X_test=X_test.values\n",
    "y_test=y_test.values\n",
    "test_data=test_data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype(int)\n",
    "X_test = X_test.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,LSTM,Lambda,Flatten\n",
    "from keras.utils import np_utils\n",
    "\n",
    "y_train_bin=np_utils.to_categorical(y_train)\n",
    "y_test_bin=np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test= scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model\n",
    "model=Sequential()\n",
    "model.add(Dense(X_train.shape[1],input_dim=X_train.shape[1],kernel_initializer='normal',bias_initializer='TruncatedNormal',activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(X_train.shape[1]//2,kernel_initializer='normal',bias_initializer='TruncatedNormal',activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(2,kernel_initializer='normal',activation='softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train,y_train_bin,epochs=200,batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "val=model.predict_proba(X_test)[:,1]\n",
    "print('ROC accuracy: {}'.format(roc_auc_score(y_test, val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred=model.predict(X_test)\n",
    "y_pred =(y_pred>0.5)[:,1]*1\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/DataSets/Kaggle/cis-Fraud/sample_submission.csv')\n",
    "\n",
    "xgb_submission=sample_submission.copy()\n",
    "xgb_submission['isFraud'] = model.predict_proba(test_data)[:,1]\n",
    "\n",
    "\n",
    "from google.colab import files\n",
    "xgb_submission.to_csv('predict2.csv',index=False) \n",
    "files.download('predict2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
