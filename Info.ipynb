{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "help(numpy)\n",
    "numpy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sql\n",
    "params=quote_plus(r'Driver={SQL Server};Server='{ServerName}';DATABASE='{TableName}'')\n",
    "engine=create_engine(\"mssql+pyodbc:///?odbc_connect=%s\" % params)\n",
    "sql_string='''some SQL request'''\n",
    "data=pd.read_sql_query(sql_string,engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#post get requests\n",
    "for i in range(len(ips)):\n",
    "    urlPost='http://'+str(ips[i])+'/queryDiskGroupList'\n",
    "    urlGet='http://'+str(ips[i])+'/queryStorageDevInfo'\n",
    "    \n",
    "    payload= {'some payload'\n",
    "             } \n",
    "    \n",
    "\n",
    "    headers = {'some headers'}\n",
    "    \n",
    "    text=''\n",
    "\n",
    "    r1 = session.get(urlGet, verify=False, headers=headers,data=payload)\n",
    "    \n",
    "    text=r1.text\n",
    "    print(r1.status_code)\n",
    "    \n",
    "    start=text.find('<serialNum>')+11\n",
    "    end=text.find('</serialNum>',start)\n",
    "    start1=text.find('<model>')+7\n",
    "    end1=text.find('</model>',start)\n",
    "    \n",
    "    file=open(names[i],'a')\n",
    "    file.write('-------------------------------------------------------------------------------------'+'\\n')\n",
    "    date=datetime.datetime.now()\n",
    "    file.write(f'Дата записи: {date}'+'\\n')\n",
    "    file.write(f'IP: {ips[i]}'+'\\n')\n",
    "    file.write(f'SerialNumber: {text[start:end]}'+'\\n')\n",
    "    file.write(f'Model: {text[start1:end1]}'+'\\n')\n",
    "    file.write('-------------------------------------------------------------------------------------'+'\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cmd \n",
    "import subprocess\n",
    "\n",
    "for l in range(len(ips)):\n",
    "    cmd='SnmpWalk.exe -r:'+str(ips[l])+' -p:161 -v:2c -c:public -os:.1.3.6.1.4.1.49198.2.100.1.2 -op:.1.3.6.1.4.1.49198.2.100.1.3'\n",
    "    val=subprocess.check_output(cmd,shell=True)\n",
    "    out=val.decode('cp866',errors='ignore')\n",
    "\n",
    "    if len(out.split('\\n'))>3:\n",
    "        print(out.split('\\n')[3:len(out.split('\\n'))-2])\n",
    "        out=out.split('\\n')[3:len(out.split('\\n'))-2]\n",
    "        \n",
    "        file=open(names[l],'a')\n",
    "        file.write('-------------------------------------------------------------------------------------'+'\\n')\n",
    "        date=datetime.datetime.now()\n",
    "        file.write(f'Дата записи: {date}'+'\\n')\n",
    "        file.write(f'IP: {ips[l]}'+'\\n')\n",
    "        for i in out:\n",
    "            file.write(i+'\\n')\n",
    "        file.write('-------------------------------------------------------------------------------------'+'\\n')\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#correlation\n",
    "corrmat=train.corr()\n",
    "top_corr_features=corrmat.index[abs(corrmat['label'])>0.6]\n",
    "plt.figure(figsize=(10,10))\n",
    "g=sns.heatmap(train[top_corr_features].corr(),annot=True,cmap='RdYlGn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#проверка на смещение\n",
    "def check_skewness(col):\n",
    "    sns.distplot(train[col],fit=norm);\n",
    "    fig=plt.figure()\n",
    "    res=stats.probplot(train[col],plot=plt)\n",
    "    (mu,sigma)=norm.fit(train[col])\n",
    "check_skewness('SalePrice')\n",
    "\n",
    "#Общая проверка на сдвиг\n",
    "numeric_feats=all_data.dtypes[all_data.dtypes!='object'].index\n",
    "\n",
    "skewed_feats=all_data[numeric_feats].apply(lambda x:skew(x.dropna())).sort_values(ascending=False)\n",
    "skewness=pd.DataFrame({'Skew':skewed_feats})\n",
    "skewness.head(18)\n",
    "\n",
    "#Последующий сдвиг\n",
    "\n",
    "skewness=skewness[abs(skewness)>0.75]\n",
    "print('There are {} num features to Box Cox'.format(skewness.shape[0]))\n",
    "\n",
    "from scipy.special import boxcox1p\n",
    "skewed_features=skewness.index\n",
    "lam=0.15\n",
    "for feat in skewed_features:\n",
    "    all_data[feat]=boxcox1p(all_data[feat],lam)\n",
    "\n",
    "#логарифмируем для предотвращения смещения на графике \n",
    "train['SalePrice']=np.log1p(train['SalePrice'])\n",
    "\n",
    "check_skewness('SalePrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#проверка на процент пропущенных значений\n",
    "\n",
    "all_data_na=(all_data.isnull().sum()/len(all_data))*100\n",
    "all_data_na=all_data_na.drop(all_data_na[all_data_na==0].index).sort_values(ascending=False)[:30]\n",
    "missing_data=pd.DataFrame({'Missing Ratio' :all_data_na})\n",
    "\n",
    "f,ax=plt.subplots(figsize=(15,12))\n",
    "plt.xticks(rotation='90')\n",
    "sns.barplot(x=all_data_na.index,y=all_data_na)\n",
    "plt.xlabel('Features',fontsize=15)\n",
    "plt.ylabel('Percent of missong values',fontsize=15)\n",
    "plt.title('Percent missing data by feature',fontsize=15)\n",
    "\n",
    "#процент пропусков без гр\n",
    "\n",
    "all_data_na=(all_data.isnull().sum()/len(all_data))*100\n",
    "all_data_na=all_data_na.drop(all_data_na[all_data_na==0].index).sort_values(ascending=False)\n",
    "missing_data=pd.DataFrame({'Missing Ratio' :all_data_na})\n",
    "missing_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Удаление строк\n",
    "dataset=dataset.drop(dataset[dataset[\"ГОД МЕСЯЦ\"]>201712].index)\n",
    "#Замена значений\n",
    "dataset = dataset.replace({'НАКАЗАНИЕ':{\"-\":0,\"Увольнение\":3,\"Выговор\":2,\"Замечание\":1,\"Взыскание\":4}})\n",
    "dataset[\"ВОЗРАСТ\"][dataset[\"ВОЗРАСТ\"]>0.085]=1\n",
    "dataset[\"ВОЗРАСТ\"][(dataset[\"ВОЗРАСТ\"]<60) & (dataset[\"ВОЗРАСТ\"]>=50)]=4\n",
    "dataset[\"ВОЗРАСТ\"]=dataset[\"ВОЗРАСТ\"].apply(lambda x: 1 if x>50 else 0)\n",
    "#Замена на медиану \n",
    "dataset['LotFrontage']=dataset.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n",
    "#Работа с датой\n",
    "data[\"PosleVida4i\"]=(data['OPDAY'] -data['IssueDate']).dt.days\n",
    "#объединение столбцов\n",
    "data_scale['new1']=list(zip(data_scale['Ssum30Perc'],data_scale['Ssum60Perc'],data_scale['Ssum90Perc']))\n",
    "#поиск индекса по значению\n",
    "X[X['Date']=='декабр'].index[0]\n",
    "#Удаление пробелов (всех)\n",
    "X['Date']=X['Date'].apply(lambda x: re.sub(r'\\s+','',x))\n",
    "#Дата в виде количества лет\n",
    "START_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')\n",
    "big_data['DT_M'] = big_data['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n",
    "big_data['DT_M'] = (big_data['DT_M'].dt.year-2017)*12 + big_data['DT_M'].dt.month \n",
    "#Объединение фреймов\n",
    "all_data=pd.concat((train,test)).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Нормализация\n",
    "from sklearn import preprocessing\n",
    "x=preprocessing.normalize(X_train)\n",
    "x=preprocessing.scale(x)\n",
    "#another norm\n",
    "train = (train - train.mean()) / (train.max() - train.min())\n",
    "#scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc=StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_validation=sc.transform(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dump\n",
    "filename=\"Scaler\"\n",
    "pickle.dump(sc,open(filename,\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#array to pd\n",
    "pred=pd.Series(np.array(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GridSearch проверка параметров модели\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "param_test ={'num_leaves': sp_randint(3, 20),\n",
    "             'boosting_type': ['goss'],\n",
    "             'max_depth':[4,5,6,7,8,9,10]\n",
    "             'min_child_samples': sp_randint(100, 500), \n",
    "             'min_child_weight': [1e-2, 1e-1, 1, 1e1, 1e2],\n",
    "             'subsample': [1,0.9,0.8,0.7,0.6], \n",
    "             'colsample_bytree':[1,0.9,0.8,0.7,0.6],\n",
    "             'reg_alpha': [0, 1e-1, 1e-2, 1e-3, 1e-4],\n",
    "             'reg_lambda': [0, 1e-1, 1e-2, 1e-3, 1e-4],\n",
    "}\n",
    "\n",
    "n = 100\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "clf = lgb.LGBMClassifier(max_depth=-1, random_state=42, silent=True, metric='None', n_jobs=4, n_estimators=1000)\n",
    "gs = RandomizedSearchCV(\n",
    "    estimator=clf, param_distributions=param_test, \n",
    "    n_iter=n,\n",
    "    scoring='roc_auc',\n",
    "    cv=3,\n",
    "    refit=True,\n",
    "    random_state=42,\n",
    "    verbose=True)\n",
    "\n",
    "gs.fit(X_train, y_train, **fit_params)\n",
    "print('Best score reached: {} with params: {} '.format(gs.best_score_, gs.best_params_))\n",
    "\n",
    "\n",
    "\n",
    "#GridSearch with catboost\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform as sp_randFloat\n",
    "from scipy.stats import randint as sp_randInt    \n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_data, \n",
    "    y, \n",
    "    test_size=0.3,\n",
    "    random_state=42, \n",
    "    shuffle=True)\n",
    "    # load the iris datasets\n",
    "\n",
    "model = CatBoostClassifier()\n",
    "parameters = {'depth'         : sp_randInt(4, 10),\n",
    "              'learning_rate' : [0.05,0.001,0.01,0.1,0.2,0.3],\n",
    "              'iterations'    : sp_randInt(10, 1000),\n",
    "              'l2_leaf_reg'   : [3,1,5,10,100],\n",
    "                 }\n",
    "    \n",
    "randm = RandomizedSearchCV(estimator=model, param_distributions = parameters, \n",
    "                               cv = 3, n_iter = 10, n_jobs=-1)\n",
    "randm.fit(X_train, y_train)\n",
    "\n",
    "    # Results from Random Search\n",
    "print(\"\\n========================================================\")\n",
    "print(\" Results from Random Search \" )\n",
    "print(\"========================================================\")    \n",
    "    \n",
    "print(\"\\n The best estimator across ALL searched params:\\n\",\n",
    "          randm.best_estimator_)\n",
    "    \n",
    "print(\"\\n The best score across ALL searched params:\\n\",\n",
    "          randm.best_score_)\n",
    "    \n",
    "print(\"\\n The best parameters across ALL searched params:\\n\",\n",
    "          randm.best_params_)\n",
    "    \n",
    "print(\"\\n ========================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Over under sampling\n",
    "#SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smt=SMOTE()\n",
    "datax,datay=smt.fit_sample(datax,datay)\n",
    "#NearMiss\n",
    "from imblearn.under_sampling import  NearMiss\n",
    "smt=NearMiss()\n",
    "datax,datay=smt.fit_sample(datax,datay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Факторный анализ\n",
    "from sklearn.decompositionposition import FactorAnalysis\n",
    "\n",
    "df_scaled=preprocessing.scale(df)\n",
    "fa=FactorAnalysis(n_components=2)\n",
    "fa.fit(df_scaled)\n",
    "\n",
    "pd.Dataframe(fa.components_,columns=df.columns\n",
    "\n",
    "fa.noise_variance_\n",
    "\n",
    "#pca\n",
    "from sklearn import decomposition             \n",
    "pca=decomposition.PCA(n_components=2)\n",
    "pca.fit(X_centered)\n",
    "X_pca=pca.transform(X_centered) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CountVec\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word', \n",
    "                             ngram_range=(1, 1),\n",
    "                             max_df=1.0,\n",
    "                             min_df=1)\n",
    "vectorizer.fit(train_data['CG1'])\n",
    "train_w=vectorizer.transform(train_data['CG1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hstack with sparse\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "tr_features=hstack((train_w,train_w2,train_w3,\n",
    "                    np.array(train['index'])[:,None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1d9d8107bb54>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mencoded\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'C7'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'C7'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mencoded\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'C7_0'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'C7_1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'C7_2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#one hot\n",
    "\n",
    "#1\n",
    "encoded=pd.get_dummies(train_data['C7'])\n",
    "train_data=train_data.drop(['C7'],axis=1)\n",
    "encoded.columns = ['C7_0','C7_1','C7_2']\n",
    "train_data = train_data.join(encoded)\n",
    "\n",
    "#nesk\n",
    "forencode=big_data.select_dtypes('object')\n",
    "\n",
    "encoded=pd.get_dummies(forencode)\n",
    "\n",
    "#drop needed col\n",
    "for i in list(forencode):\n",
    "    big_data = big_data.drop(i,axis = 1)\n",
    "# join the encoded df\n",
    "\n",
    "big_data = big_data.join(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#label\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labEnc1=LabelEncoder()\n",
    "labEnc2=LabelEncoder()\n",
    "datasetX[\"ГРАФИК РАБОЧЕГО ВРЕМЕНИ\"]=labEnc1.fit_transform(datasetX[\"ГРАФИК РАБОЧЕГО ВРЕМЕНИ\"])\n",
    "datasetX[\"ГРУППА НАКАЗАНИЙ\"]=labEnc2.fit_transform(datasetX[\"ГРУППА НАКАЗАНИЙ\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model with KFold's and predict\n",
    "\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit, StratifiedKFold\n",
    "import lightgbm as lgb\n",
    "\n",
    "n_fold = 10\n",
    "#folds = KFold(n_splits=n_fold,shuffle=True,random_state=42)\n",
    "folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "sample_submission = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/DataSets/Kaggle/cis-Fraud/sample_submission.csv')\n",
    "\n",
    "xgb_submission=sample_submission.copy()\n",
    "xgb_submission['isFraud'] = 0\n",
    "t=0\n",
    "\n",
    "mean=[]\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "#for fold_n, (train_index, valid_index) in enumerate(folds.split(train_data)):\n",
    "for train_index, valid_index in folds.split(train_data, y):\n",
    "  #if t<3:\n",
    "    lgbR=lgb.LGBMClassifier(\n",
    "                objective='binary',\n",
    "                boosting_type='gbdt',\n",
    "                metric='auc',\n",
    "                n_jobs=-1,\n",
    "                learning_rate=0.064,\n",
    "                num_leaves= 2**8,\n",
    "                min_data_in_leaf=1000,\n",
    "                #num_iterations=15000,\n",
    "                max_depth=10,\n",
    "                tree_learner='serial',\n",
    "                colsample_bytree= 0.85,\n",
    "                subsample_freq=1,\n",
    "                subsample=0.85,\n",
    "                n_estimators=2**9,\n",
    "                max_bin=255,\n",
    "                verbose=-1,\n",
    "                seed= 47,\n",
    "                early_stopping_rounds=100,\n",
    "                reg_alpha=0.3,\n",
    "                reg_lamdba=0.243\n",
    "      )\n",
    "    \n",
    "    X_train_, X_valid = train_data.iloc[train_index], train_data.iloc[valid_index]\n",
    "    y_train_, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "    lgbR.fit(X_train_,y_train_,eval_metric='auc',eval_set=[(X_valid,y_valid)],verbose=50,early_stopping_rounds=100)\n",
    "    del X_train_,y_train_\n",
    "    pred=lgbR.predict_proba(test_data)[:,1]\n",
    "    val=lgbR.predict_proba(X_valid)[:,1]\n",
    "    del X_valid\n",
    "    del lgbR\n",
    "    print('ROC accuracy: {}'.format(roc_auc_score(y_valid, val)))\n",
    "    mean.append(roc_auc_score(y_valid, val))\n",
    "    del val,y_valid\n",
    "    xgb_submission['isFraud'] = xgb_submission['isFraud']+pred/n_fold\n",
    "    del pred\n",
    "    t=t+1\n",
    "\n",
    "m=0\n",
    "for i in mean:\n",
    "    m=m+i/len(mean)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#feature importance\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# sorted(zip(clf.feature_importances_, X.columns), reverse=True)\n",
    "feature_imp = pd.DataFrame(sorted(zip(lgbR.feature_importances_,test_data.columns)), columns=['Value','Feature'])\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\n",
    "plt.title('LightGBM Features (avg over folds)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ONE HOT PEREDELKA\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "#test one hot\n",
    "features=datasetX.copy()\n",
    "\n",
    "num_sub=datasetX.select_dtypes(include=[\"number\"])\n",
    "\n",
    "\n",
    "#columns with log of num col\n",
    "for col in num_sub.columns:\n",
    "    num_sub[\"log_\" + col]=np.exp(num_sub[col])\n",
    "\n",
    "#col with categor\n",
    "cat_sub=datasetX1[[\"ГРАФИК РАБОЧЕГО ВРЕМЕНИ\",\"ГРУППА НАКАЗАНИЙ\",\"МЕСЯЦ\",\"СТЕПЕНЬ ЗАНЯТОСТИ\",\"ВОЗРАСТ\"]]\n",
    "\n",
    "\n",
    "#one hot\n",
    "cat_sub=pd.get_dummies(cat_sub)\n",
    "\n",
    "#join dataframes\n",
    "features=pd.concat([num_sub,cat_sub],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
